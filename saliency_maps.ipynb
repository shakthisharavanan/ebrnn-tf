{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for spatio-temporal Maps using ebrnn\n",
    "## Author: Shakthi Duraimurugan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from os import path\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "import pylab as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm, trange, tqdm_notebook, tnrange\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "import subprocess as sp\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_dir = \"/mnt/workspace/models/research/slim/\"\n",
    "checkpoints_dir = \"/mnt/workspace/models/checkpoints/\"\n",
    "sys.path.insert(0, slim_dir)\n",
    "from nets import vgg\n",
    "from preprocessing import vgg_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define EBRNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ebrnn(object):\n",
    "    def __init__(self, image_size, batch_size, image_mean):\n",
    "        # Set Paths\n",
    "        self.dataset_dir = \"/mnt/workspace/datasets/UCF-101/\"\n",
    "        self.label_dir = \"/mnt/workspace/datasets/ucf101/ucf24/labels/\"\n",
    "        self.checkpoints_dir = \"/mnt/workspace/models/checkpoints/\"\n",
    "        self.extracted_features_dir = \"/mnt/workspace/ebrnn-tf/fc7_features/\"\n",
    "\n",
    "        self.labels = [x.replace(self.label_dir,\"\") for x in sorted(glob.glob(self.label_dir+\"*\"))]\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.image_mean = image_mean\n",
    "\n",
    "        # Define the VGG 16 Network\n",
    "        g1 = tf.Graph()\n",
    "        slim = tf.contrib.slim\n",
    "        with g1.as_default():\n",
    "            self.input_batch = tf.placeholder(dtype=tf.uint8, shape=(batch_size,240,320,3))\n",
    "            resized_images = tf.image.resize_images(self.input_batch, [self.image_size,self.image_size])\n",
    "            channels = tf.split(axis=3, num_or_size_splits=3, value=resized_images)\n",
    "            for i in range(3):\n",
    "                channels[i] -= self.image_mean[i]\n",
    "            normalized_images = tf.concat(axis=3, values=channels)\n",
    "\n",
    "            with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "                outputs, end_points = vgg.vgg_16(normalized_images,num_classes=1000, is_training=False)\n",
    "\n",
    "            self.final_conv = end_points['vgg_16/conv5/conv5_3']\n",
    "            self.fc7 = end_points['vgg_16/fc7']\n",
    "            self.probablities = tf.nn.softmax(outputs)\n",
    "\n",
    "            self.vgg_sess = tf.Session()\n",
    "\n",
    "\n",
    "\n",
    "        # Define LSTM model\n",
    "        g2 = tf.Graph()\n",
    "        with g2.as_default():\n",
    "            # Define placeholders\n",
    "            x = tf.placeholder(tf.float32, (1, n_frames, 4096))\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, activation = tf.nn.relu)\n",
    "            seq_out, states = tf.nn.dynamic_rnn(cell = lstm_cell, sequence_length = sequence_length, inputs = x, dtype = tf.float32)\n",
    "            w = tf.get_variable(name = \"lstm_weights\", shape = [n_hidden, n_classes], \n",
    "                    initializer = tf.contrib.layers.xavier_initializer(), trainable = True)\n",
    "            b = tf.get_variable(name = \"lstm_bias\", shape = [n_classes], \n",
    "                    initializer = tf.contrib.layers.xavier_initializer(), trainable = True)\n",
    "            final_out = seq_out[:, -1, :] # last time step's output\n",
    "            out = tf.matmul(final_out, w) + b\n",
    "            y = tf.placeholder(tf.uint8, (None))\n",
    "            y_one_hot = tf.one_hot(y, len(labels))\n",
    "            sequence_length = tf.placeholder(tf.int32, shape=None)\n",
    "            logits = lstm_model(x, batch_size, sequence_length, n_hidden, len(labels))\n",
    "            self.lstm_sess = tf.Session()\n",
    "\n",
    "    def read_video(self, video_path):\n",
    "        command = [ 'ffmpeg', '-i', video_path, '-f', 'image2pipe', '-pix_fmt', 'rgb24', '-vcodec', 'rawvideo', '-']\n",
    "        pipe = sp.Popen(command, stdout = sp.PIPE, stderr = open(os.devnull, 'w'), bufsize=10**8)\n",
    "        while True:\n",
    "            raw_image = pipe.stdout.read(240*320*3)\n",
    "            if len(raw_image) != 240*320*3:\n",
    "                break;\n",
    "            # transform the byte read into a numpy array\n",
    "            image =  np.fromstring(raw_image, dtype='uint8')\n",
    "            image = image.reshape((240,320,3))\n",
    "            self.video_frames.append(image)\n",
    "            # throw away the data in the pipe's buffer.\n",
    "            pipe.stdout.flush()\n",
    "        self.video_frames = np.asarray(self.video_frames, dtype=np.uint8)\n",
    "        \n",
    "        \n",
    "    def get_imagenet_features(self, label, video_path):\n",
    "        # Read frames from the video\n",
    "        self.video_frames = []\n",
    "        self.read_video(video_path)\n",
    "        n = self.video_frames.shape[0]\n",
    "\n",
    "        if not(os.path.exists(self.extracted_features_dir + label)):\n",
    "            os.mkdir(self.extracted_features_dir + label)\n",
    "\n",
    "        filename = self.extracted_features_dir + label + \"/\" + video_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "        # Iterate through batches and extract features\n",
    "        full_video_features = []\n",
    "        for start, end in zip(range(0, n, self.batch_size), range(self.batch_size, n + self.batch_size, self.batch_size)):\n",
    "            current_batch = np.zeros((self.batch_size, 240, 320, 3), dtype = np.uint8)\n",
    "            current_batch[:min(end, n) - start] = self.video_frames[start:end]\n",
    "\n",
    "            # final_conv = self.sess.run(self.final_conv, feed_dict = {self.input_batch: current_batch})\n",
    "            fc7 = self.sess.run(self.fc7, feed_dict = {self.input_batch: current_batch})\n",
    "            full_video_features = full_video_features + list(fc7)\n",
    "\n",
    "        features = np.asarray(full_video_features[:n], np.float32) \n",
    "        np.save(filename, features)\n",
    "        # pdb.set_trace()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
